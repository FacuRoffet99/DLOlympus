{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gitlab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \n",
    "email = \n",
    "username = \n",
    "repo = \n",
    "id = \n",
    "branch = \n",
    "experiment =\n",
    "\n",
    "!git config --global user.email {email}\n",
    "!git config --global user.name {username}\n",
    "\n",
    "import os\n",
    "os.environ['MLFLOW_TRACKING_URI'] = f'https://gitlab.com/api/v4/projects/{id}/ml/mlflow'\n",
    "os.environ['MLFLOW_TRACKING_TOKEN'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone -b {branch} https://oauth2:{token}@gitlab.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "try:\n",
    "  import google.colab\n",
    "  google.colab.drive.mount('/content/drive')\n",
    "  root = 'path_to_main_folder/'\n",
    "except:\n",
    "  root= 'path_to_main_folder/'\n",
    "images_folder = root+'data/images/'\n",
    "save_path = root+'models/'\n",
    "other_paths = root+'declare_other_paths/'\n",
    "\n",
    "task_type = 'classification'\n",
    "task_names = ['a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import albumentations\n",
    "from DLOlympus.training.transforms import AlbumentationsTransform\n",
    "from DLOlympus.training.utils import get_model\n",
    "from DLOlympus.training.unbalanced import get_weights, oversampled_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "h, w = 224, 224\n",
    "\n",
    "hyperparameters = {\n",
    "    'MODEL_DESCRIPTION': '',\n",
    "    'BS': 16,\n",
    "    'EPOCHS': 30,\n",
    "    'IMG_SIZE': (h, w),      # (height, width)\n",
    "    'WD': 0.0,\n",
    "    'TRANSFORMS': [\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.Rotate(p=0.5),\n",
    "        albumentations.Sharpen(p=0.5),\n",
    "        albumentations.ColorJitter(brightness=0.3, contrast=0.5, saturation=0.5, hue=0.0, p=0.5),\n",
    "        albumentations.RGBShift(p=0.5),\n",
    "        albumentations.GaussianBlur(p=0.5),\n",
    "        albumentations.GaussNoise(p=0.5),\n",
    "        albumentations.RandomSizedCrop((int(0.75*h),h), h, w, p=1.0)\n",
    "        ],\n",
    "    'ARCH': 'resnet50',\n",
    "    'ARCH_TYPE': 'torchvision',\n",
    "    'LOSS_FUNC': 'LabelSmoothingCrossEntropyFlat',\n",
    "    'OPT_FUNC': 'Adam',\n",
    "    'USE_OVERSAMPLING': False,\n",
    "    'USE_LOSS_WEIGHTS': False,\n",
    "    'SEED': 18,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "def get_data():\n",
    "    image_files = \n",
    "    labels = \n",
    "    groups = \n",
    "    return image_files, labels, groups\n",
    "\n",
    "def create_df(image_files, labels, groups, n_splits=10, n_valid=2):\n",
    "    # Initiate dataframe\n",
    "    df = pd.DataFrame()\n",
    "    df['file_path'] = image_files\n",
    "    df['label'] = labels\n",
    "    df['groups'] = groups\n",
    "    df['fold'] = -1\n",
    "    # Make folds\n",
    "    cv = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    for i, (train_idxs, valid_idxs) in enumerate(cv.split(image_files, labels, groups)):\n",
    "        df.loc[valid_idxs, ['fold']] = i\n",
    "    # Assign folds for validation\n",
    "    df['split'] = 'train'\n",
    "    for i in range (n_valid):\n",
    "        df.loc[df.fold == i, ['split']] = 'valid'\n",
    "    del df['fold']\n",
    "    df.split.value_counts()\n",
    "    # Add a binary column to the dataframe\n",
    "    df['is_valid'] = df.split == 'valid'\n",
    "    del df['split']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "image_files, labels, groups = get_data()\n",
    "df = create_df(image_files, labels, groups)\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hyperparameters['SEED'], True)\n",
    "\n",
    "# Determine the number of tasks\n",
    "n_tasks = len(task_names)\n",
    "\n",
    "# Determine block types\n",
    "if task_type == 'classification':\n",
    "\tblocks = (ImageBlock,) + (CategoryBlock,) * n_tasks \n",
    "if task_type == 'regression':\n",
    "\tblocks = (ImageBlock, RegressionBlock(n_out=n_tasks))\n",
    "\t\n",
    "# Datablock\n",
    "block = DataBlock(\n",
    "    blocks=blocks,\n",
    "    n_inp=1,\n",
    "    get_x=ColReader('file_path'),\n",
    "    get_y=[lambda x, i=i: x['label'].split(', ')[i] for i in range(n_tasks)],\n",
    "    splitter=ColSplitter(col='is_valid'),\n",
    "    item_tfms=[\n",
    "        Resize(hyperparameters['IMG_SIZE'], method='squish'), \n",
    "        AlbumentationsTransform(albumentations.Compose(hyperparameters['TRANSFORMS']))])\n",
    "\n",
    "# Dataloaders\n",
    "dls = block.dataloaders(df, bs=hyperparameters['BS'], shuffle=True)\n",
    "dls.rng.seed(hyperparameters['SEED'])\n",
    "\n",
    "# Sanity check\n",
    "if task_type == 'classification':\n",
    "\tn_classes = [dls.c] if isinstance(dls.c, int) else dls.c\n",
    "\tprint('Number of clases: ', n_classes)\n",
    "\tclasses = dls.vocab if n_tasks>1 else [dls.vocab]\n",
    "\tprint('Names of classes: ', classes)\n",
    "if task_type == 'regression':\n",
    "\tn_out = dls.c \n",
    "\tprint('Number of outputs: ', n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show batch\n",
    "dls.train.show_batch(max_n=16, figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show transforms\n",
    "dls.train.show_batch(max_n=16, unique=True, figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "loss_weights = get_weights(dls) if hyperparameters['USE_LOSS_WEIGHTS'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.training.metrics import AccuracyMetric, F1ScoreMetric, MSEMetric\n",
    "from DLOlympus.training.multitask import create_loss_func\n",
    "\n",
    "if task_type == 'classification':\n",
    "\tif n_tasks == 1:\n",
    "\t\tloss = getattr(sys.modules[__name__], hyperparameters['LOSS_FUNC'])(weight=loss_weights)\n",
    "\t\tmetrics = [AccuracyMetric(), F1ScoreMetric(average='macro')]\n",
    "\t\tcallbacks = [SaveModelCallback(monitor='f1_score', with_opt=True), ShowGraphCallback]\n",
    "\telse:\n",
    "\t\tloss_functions = [create_loss_func(getattr(sys.modules[__name__], hyperparameters['LOSS_FUNC'])(weight=loss_weights), sum(n_classes[:i]), sum(n_classes[:i+1]), i) for i in range(n_tasks)]    \n",
    "\t\tdef combined_loss(inp, *args): \n",
    "\t\t\treturn sum(f(inp, *args) for f in loss_functions)\n",
    "\t\tloss = combined_loss\n",
    "\t\tacc_functions = [AccuracyMetric(axis=i, metric_name='acc_'+t) for i,t in enumerate(task_names)]\n",
    "\t\tf1_functions = [F1ScoreMetric(axis=i, metric_name='f1_'+t) for i,t in enumerate(task_names)]\n",
    "\t\tmetrics = acc_functions + f1_functions + [AccuracyMetric(multi=True, metric_name='acc_multi')]\n",
    "\t\tcallbacks = [SaveModelCallback(monitor='acc_multi', with_opt=True), ShowGraphCallback]\n",
    "if task_type == 'regression':\n",
    "\tloss = getattr(sys.modules[__name__], hyperparameters['LOSS_FUNC'])()\n",
    "\tcallbacks = [SaveModelCallback(monitor='valid_loss', comp=np.less, with_opt=True), ShowGraphCallback]\n",
    "\tif n_tasks == 1:\n",
    "\t\tmetrics = [MSEMetric(metric_name='rmse', root=True)]\n",
    "\telse:\n",
    "\t\tmetrics = [MSEMetric(metric_name='rmse_'+t, axis=i, root=True) for i,t in enumerate(task_names)]\n",
    "\t\t\n",
    "# Learner\n",
    "learn = vision_learner(dls,\n",
    "                        get_model(hyperparameters),\n",
    "                        normalize=True,\n",
    "                        pretrained=True,\n",
    "                        n_out=sum(n_classes),\n",
    "                        loss_func=loss,\n",
    "                        opt_func=getattr(sys.modules[__name__], hyperparameters['OPT_FUNC']),\n",
    "                        metrics=metrics,\n",
    "                        wd=hyperparameters['WD']).to_fp16()\n",
    "\n",
    "# Fix issue with pickling while calling learn.export\n",
    "import typing, functools\n",
    "learn.loss_func.func.__annotations__ = typing.get_type_hints(learn.loss_func.func, globalns=globals(), localns=locals())\n",
    "functools.update_wrapper(learn.loss_func, learn.loss_func.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "if hyperparameters['USE_OVERSAMPLING']:\n",
    "    class_weights = pd.DataFrame(1 / np.sqrt(learn.dls.items.label.value_counts())).rename(index=lambda x: str(x)).to_dict()['count']\n",
    "    learn.dls.train.get_idxs = types.MethodType(partial(oversampled_epoch, class_weights=class_weights), learn.dls.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find LR\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LR\n",
    "hyperparameters['LR'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "learn.fine_tune(hyperparameters['EPOCHS'], base_lr=hyperparameters['LR'], cbs=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "learn.export(f'{save_path}/model.pkl', pickle_module=dill)\n",
    "learn.save(f'{save_path}/model')\n",
    "\n",
    "from DLOlympus.training.plots import plot_losses, plot_metrics\n",
    "_ = plot_losses(learn, save_path)\n",
    "_ = plot_metrics(learn, save_path)\n",
    "\n",
    "if task_type == 'classification':\n",
    "\timport itertools\n",
    "\tfrom DLOlympus.training.plots import plot_confusion_matrix\n",
    "\tprobs, ground_truths = learn.get_preds(ds_idx=1)        # DO NOT PREDICT BEFORE PLOTTING LOSSES AND METRICS\n",
    "\tground_truths = ground_truths if n_tasks>1 else [ground_truths]\n",
    "\tpredictions = [np.argmax(probs[:,sum(n_classes[:i]):sum(n_classes[:i+1])], axis=1) for i in range(n_tasks)]\n",
    "\tdecoded_preds = [' '.join([classes[i][p] for i, p in enumerate(tensor(g))]) for g in zip(*predictions)]\n",
    "\tdecoded_gts = [' '.join([classes[i][p] for i, p in enumerate(tensor(g))]) for g in zip(*ground_truths)]\n",
    "\tnew_vocab = [' '.join(i) for i in list(itertools.product(*classes))]\n",
    "\t_ = plot_confusion_matrix(decoded_gts, decoded_preds, new_vocab, save_path)\n",
    "\n",
    "if n_tasks==1:\n",
    "    from DLOlympus.training.tables import get_predictions_table\n",
    "    train_table = get_predictions_table(learn, learn.dls.train)\n",
    "    valid_table = get_predictions_table(learn, learn.dls.valid)\n",
    "    train_table.to_csv(f'{save_path}train_table.csv', index=False)\n",
    "    valid_table.to_csv(f'{save_path}valid_table.csv', index=False)\n",
    "\n",
    "from DLOlympus.training.utils import get_metrics\n",
    "results = get_metrics(learn, with_tta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(str([t.__class__.__name__ for t in hyperparameters['TRANSFORMS']])) > 250:\n",
    "    hyperparameters['TRANSFORMS'] = 'Too many transforms to log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.training.mlflow import mlflow_log\n",
    "\n",
    "mlflow_log(save_path, hyperparameters, results, experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multicare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
