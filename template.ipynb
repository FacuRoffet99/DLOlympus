{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gitlab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \n",
    "email = \n",
    "username = \n",
    "repo = \n",
    "id = \n",
    "branch = \n",
    "experiment =\n",
    "\n",
    "!git config --global user.email {email}\n",
    "!git config --global user.name {username}\n",
    "\n",
    "import os\n",
    "os.environ['MLFLOW_TRACKING_URI'] = f'https://gitlab.com/api/v4/projects/{id}/ml/mlflow'\n",
    "os.environ['MLFLOW_TRACKING_TOKEN'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone -b {branch} https://oauth2:{token}@gitlab.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "root = 'path_to_main_folder/'\n",
    "images_folder = root+'data/images/'\n",
    "save_path = root+'models/'\n",
    "other_paths = root+'declare_other_paths/'\n",
    "\n",
    "task_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import albumentations\n",
    "from DLOlympus.training.transforms import AlbumentationsTransform\n",
    "from DLOlympus.training.utils import get_model\n",
    "from DLOlympus.training.unbalanced import get_weights, oversampled_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "h, w = 224, 224\n",
    "\n",
    "hyperparameters = {\n",
    "    'BS': 16,\n",
    "    'EPOCHS': 30,\n",
    "    'IMG_SIZE': (h, w),      # (height, width)\n",
    "    'WD': 0.0,\n",
    "    'TRANSFORMS': [\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.Rotate(p=0.5),\n",
    "        albumentations.Sharpen(p=0.5),\n",
    "        albumentations.ColorJitter(brightness=0.3, contrast=0.5, saturation=0.5, hue=0.0, p=0.5),\n",
    "        albumentations.RGBShift(p=0.5),\n",
    "        albumentations.GaussianBlur(p=0.5),\n",
    "        albumentations.GaussNoise(p=0.5),\n",
    "        albumentations.RandomSizedCrop((int(0.75*h),h), h, w, p=1.0)\n",
    "        ],\n",
    "    'ARCH': 'resnet50',\n",
    "    'ARCH_TYPE': 'torchvision',\n",
    "    'LOSS_FUNC': 'LabelSmoothingCrossEntropyFlat',\n",
    "    'OPT_FUNC': 'Adam',\n",
    "    'USE_OVERSAMPLING': False,\n",
    "    'USE_LOSS_WEIGHTS': False,\n",
    "    'SEED': 18,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "def get_data():\n",
    "    image_files = \n",
    "    labels = \n",
    "    groups = \n",
    "    return image_files, labels, groups\n",
    "\n",
    "def create_df(image_files, labels, groups, n_splits=10, n_valid=2):\n",
    "    # Initiate dataframe\n",
    "    df = pd.DataFrame()\n",
    "    df['file_path'] = image_files\n",
    "    df['label'] = labels\n",
    "    df['groups'] = groups\n",
    "    df['fold'] = -1\n",
    "    # Make folds\n",
    "    cv = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    for i, (train_idxs, valid_idxs) in enumerate(cv.split(image_files, labels, groups)):\n",
    "        df.loc[valid_idxs, ['fold']] = i\n",
    "    # Assign folds for validation\n",
    "    df['split'] = 'train'\n",
    "    for i in range (n_valid):\n",
    "        df.loc[df.fold == i, ['split']] = 'valid'\n",
    "    del df['fold']\n",
    "    df.split.value_counts()\n",
    "    # Add a binary column to the dataframe\n",
    "    df['is_valid'] = df.split == 'valid'\n",
    "    del df['split']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "image_files, labels, groups = get_data()\n",
    "df = create_df(image_files, labels, groups)\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hyperparameters['SEED'], True)\n",
    "\n",
    "# Determine the number of tasks\n",
    "n_tasks = len(df.label[0].split(', '))\n",
    "\n",
    "# Datablock\n",
    "block = DataBlock(\n",
    "    blocks=(ImageBlock,) + (CategoryBlock,) * n_tasks,\n",
    "    n_inp=1,\n",
    "    get_x=ColReader('file_path'),\n",
    "    get_y=[lambda x, i=i: x['label'].split(', ')[i] for i in range(n_tasks)],\n",
    "    splitter=ColSplitter(col='is_valid'),\n",
    "    item_tfms=[\n",
    "        Resize(hyperparameters['IMG_SIZE'], method='squish'), \n",
    "        AlbumentationsTransform(albumentations.Compose(hyperparameters['TRANSFORMS']))])\n",
    "\n",
    "# Dataloaders\n",
    "dls = block.dataloaders(df, bs=hyperparameters['BS'], shuffle=True)\n",
    "dls.rng.seed(hyperparameters['SEED'])\n",
    "\n",
    "# Sanity check\n",
    "n_classes = dls.c if n_tasks>1 else [dls.c] \n",
    "classes = dls.vocab if n_tasks>1 else [dls.vocab]\n",
    "print('Number of clases: ', n_classes)\n",
    "print('Names of classes: ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show batch\n",
    "dls.train.show_batch(max_n=16, figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show transforms\n",
    "dls.train.show_batch(max_n=16, unique=True, figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "loss_weights = get_weights(dls) if hyperparameters['USE_LOSS_WEIGHTS'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(n_classes) == 1:\n",
    "    loss = getattr(sys.modules[__name__], hyperparameters['LOSS_FUNC'])(weight=loss_weights)\n",
    "    metrics = [accuracy, F1Score(average='macro')]\n",
    "    callbacks = [SaveModelCallback(monitor='f1_score', with_opt=True), ShowGraphCallback]\n",
    "else:\n",
    "    from DLOlympus.training.multitask import create_loss_func, create_acc_func, multi_acc\n",
    "    # Create a loss function and an accuracy for each task\n",
    "    loss_functions = [create_loss_func(getattr(sys.modules[__name__], hyperparameters['LOSS_FUNC'])(weight=loss_weights), sum(n_classes[:i]), sum(n_classes[:i+1]), i) for i in range(n_tasks)]    \n",
    "    acc_functions = [create_acc_func(sum(n_classes[:i]), sum(n_classes[:i+1]), i) for i in range(n_tasks)]\n",
    "    for func,n in zip(acc_functions, task_names): func.__name__ = f'{n}_acc'\n",
    "    # Sum all the losses\n",
    "    def combined_loss(inp, *args): return sum(f(inp, *args) for f in loss_functions)\n",
    "\n",
    "    loss = combined_loss\n",
    "    metrics = acc_functions + [partial(multi_acc, n_classes=n_classes)]\n",
    "    metrics[-1].__name__ = 'multi_acc'\n",
    "    callbacks = [SaveModelCallback(monitor='multi_acc', with_opt=True), ShowGraphCallback]\n",
    "\n",
    "\n",
    "# Learner\n",
    "learn = vision_learner(dls,\n",
    "                        get_model(hyperparameters),\n",
    "                        normalize=True,\n",
    "                        pretrained=True,\n",
    "                        n_out=sum(n_classes),\n",
    "                        loss_func=loss,\n",
    "                        opt_func=getattr(sys.modules[__name__], hyperparameters['OPT_FUNC']),\n",
    "                        metrics=metrics,\n",
    "                        wd=hyperparameters['WD']).to_fp16()\n",
    "\n",
    "# Fix issue with pickling while calling learn.export\n",
    "import typing, functools\n",
    "learn.loss_func.func.__annotations__ = typing.get_type_hints(learn.loss_func.func, globalns=globals(), localns=locals())\n",
    "functools.update_wrapper(learn.loss_func, learn.loss_func.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "if hyperparameters['USE_OVERSAMPLING']:\n",
    "    class_weights = pd.DataFrame(1 / np.sqrt(learn.dls.items.label.value_counts())).rename(index=lambda x: str(x)).to_dict()['count']\n",
    "    learn.dls.train.get_idxs = types.MethodType(partial(oversampled_epoch, class_weights=class_weights), learn.dls.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find LR\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LR\n",
    "hyperparameters['LR'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "learn.fine_tune(hyperparameters['EPOCHS'], base_lr=hyperparameters['LR'], cbs=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "learn.export(f'{save_path}/model.pkl', pickle_module=dill)\n",
    "learn.save(f'{save_path}/model')\n",
    "\n",
    "from DLOlympus.training.plots import plot_losses, plot_metrics\n",
    "_ = plot_losses(learn, save_path)\n",
    "_ = plot_metrics(learn, save_path)\n",
    "\n",
    "import itertools\n",
    "from DLOlympus.training.plots import plot_confusion_matrix\n",
    "probs, ground_truths = learn.get_preds(ds_idx=1)        # DO NOT PREDICT BEFORE PLOTTING LOSSES AND METRICS\n",
    "ground_truths = ground_truths if n_tasks>1 else [ground_truths]\n",
    "predictions = [np.argmax(probs[:,sum(n_classes[:i]):sum(n_classes[:i+1])], axis=1) for i in range(n_tasks)]\n",
    "decoded_preds = [' '.join([classes[i][p] for i, p in enumerate(tensor(g))]) for g in zip(*predictions)]\n",
    "decoded_gts = [' '.join([classes[i][p] for i, p in enumerate(tensor(g))]) for g in zip(*ground_truths)]\n",
    "new_vocab = [' '.join(i) for i in list(itertools.product(*classes))]\n",
    "_ = plot_confusion_matrix(decoded_gts, decoded_preds, new_vocab, save_path)\n",
    "\n",
    "if n_tasks==1:\n",
    "    from DLOlympus.training.tables import get_predictions_table\n",
    "    train_table = get_predictions_table(learn, learn.dls.train)\n",
    "    valid_table = get_predictions_table(learn, learn.dls.valid)\n",
    "    train_table.to_csv(f'{save_path}train_table.csv', index=False)\n",
    "    valid_table.to_csv(f'{save_path}valid_table.csv', index=False)\n",
    "\n",
    "from DLOlympus.training.utils import get_metrics\n",
    "results = get_metrics(learn, with_tta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(str([t.__class__.__name__ for t in hyperparameters['TRANSFORMS']])) > 250:\n",
    "    hyperparameters['TRANSFORMS'] = 'Too many transforms to log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.training.mlflow import mlflow_log\n",
    "\n",
    "mlflow_log(save_path, hyperparameters, results, experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multicare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
