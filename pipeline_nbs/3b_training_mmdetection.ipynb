{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4aa3bf",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fea9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "try:\n",
    "  import google.colab\n",
    "  google.colab.drive.mount('/content/drive')\n",
    "  root = 'path_to_main_drive_folder/'\n",
    "except:\n",
    "  root= 'path_to_main_local_folder/'\n",
    "\n",
    "data_root = \n",
    "mmdet_root = \n",
    "\n",
    "# Paths to the COCO-style jsons with all the annotations\n",
    "anno_train = data_root + 'train.json'\n",
    "anno_valid = data_root + 'valid.json'\n",
    "anno_test = data_root + 'test.json'\n",
    "\n",
    "# Folders of the images\n",
    "train_folder = data_root + 'images/train'\n",
    "valid_folder = data_root + 'images/valid'\n",
    "test_folder = data_root + 'images/test'\n",
    "\n",
    "# Working directory\n",
    "work_dir = root + 'models/'\n",
    "\n",
    "# All the classes in the dataset\n",
    "classes = ('', '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "h, w = 640, 640\n",
    "\n",
    "hyperparameters = {\n",
    "    'MODEL_DESCRIPTION': '',\n",
    "    'BS': 8,\n",
    "    'EPOCHS': 30,\n",
    "    'IMG_SIZE': (h, w),      # (height, width)\n",
    "    'LR': 5e-5,\n",
    "    # 'MOMS': (0.85, 0.95),\n",
    "    'LR_DIVS': (30, 1e4),\n",
    "    'WARMUP_PCT': 1/3,    \n",
    "    'TRANSFORMS': [\n",
    "        dict(type='RandomAffine', max_rotate_degree=5.0, max_translate_ratio=0.0, scaling_ratio_range=(1.0, 1.0), max_shear_degree=1.0),\n",
    "        dict(type='RandomCrop', crop_size=(0.8,0.8), crop_type='relative_range', allow_negative_crop=True),\n",
    "        dict(type='Resize', scale=(h, w)),\n",
    "        dict(type='RandomFlip', direction=['horizontal', 'vertical'], prob=[1/2,1/2]),\n",
    "        dict(type='PhotoMetricDistortion', brightness_delta=25, contrast_range=(0.7, 1.3), saturation_range=(0.7, 1.3), hue_delta=25),\n",
    "    ],\n",
    "    'ARCH': 'dino-4scale_r50',\n",
    "    'BASE_CONFIG': 'configs/dino/dino-4scale_r50_improved_8xb2-12e_coco.py',\n",
    "    'WEIGHTS': 'https://download.openmmlab.com/mmdetection/v3.0/dino/dino-4scale_r50_improved_8xb2-12e_coco/dino-4scale_r50_improved_8xb2-12e_coco_20230818_162607-6f47a913.pth',\n",
    "    'SEED': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9aeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "from mmengine.runner import set_random_seed\n",
    "from mmengine.runner import Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85241f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.mmdetection.hooks import ValidationLossHook\n",
    "\n",
    "# Load base config file\n",
    "cfg = Config.fromfile(mmdet_root+hyperparameters['BASE_CONFIG'])\n",
    "\n",
    "# Modify dataset classes\n",
    "cfg.metainfo = {\n",
    "    'classes': classes,\n",
    "}\n",
    "\n",
    "# Modify dataset files\n",
    "cfg.data_root = data_root\n",
    "\n",
    "if 'dataset' in cfg.train_dataloader.dataset.keys(): cfg.train_dataloader.dataset = cfg.train_dataloader.dataset.dataset # hotfix\n",
    "cfg.train_dataloader.dataset.ann_file = anno_train\n",
    "cfg.train_dataloader.dataset.data_root = data_root\n",
    "cfg.train_dataloader.dataset.data_prefix.img = train_folder\n",
    "cfg.train_dataloader.dataset.metainfo = cfg.metainfo\n",
    "cfg.train_dataloader.dataset.filter_cfg.filter_empty_gt = False\n",
    "\n",
    "cfg.val_dataloader.dataset.ann_file = anno_valid\n",
    "cfg.val_dataloader.dataset.data_root = data_root\n",
    "cfg.val_dataloader.dataset.data_prefix.img = valid_folder\n",
    "cfg.val_dataloader.dataset.metainfo = cfg.metainfo\n",
    "\n",
    "cfg.test_dataloader.dataset.ann_file = anno_test\n",
    "cfg.test_dataloader.dataset.data_root = data_root\n",
    "cfg.test_dataloader.dataset.data_prefix.img = test_folder\n",
    "cfg.test_dataloader.dataset.metainfo = cfg.metainfo\n",
    "\n",
    "# Modify metric config\n",
    "cfg.val_evaluator.ann_file = anno_valid\n",
    "cfg.val_evaluator.classwise = True\n",
    "cfg.test_evaluator.ann_file = anno_test\n",
    "cfg.test_evaluator.classwise = True\n",
    "\n",
    "# Modify num classes of each model head\n",
    "if 'bbox_head' in cfg.model:\n",
    "    cfg.model.bbox_head.num_classes = len(classes)\n",
    "if 'segm_head' in cfg.model:\n",
    "    cfg.model.mask_head.num_classes = len(classes)\n",
    "if 'roi_head' in cfg.model:\n",
    "    if 'bbox_head' in cfg.model.roi_head:\n",
    "        if 'num_classes' in cfg.model.roi_head.bbox_head:\n",
    "            cfg.model.roi_head.bbox_head.num_classes = len(classes)\n",
    "        else:\n",
    "            for head in cfg.model.roi_head.bbox_head:\n",
    "                head.num_classes = len(classes)\n",
    "    if 'segm_head' in cfg.model.roi_head:\n",
    "        if 'num_classes' in cfg.model.roi_head.segm_head:\n",
    "            cfg.model.roi_head.segm_head.num_classes = len(classes)\n",
    "        else:\n",
    "            for head in cfg.model.roi_head.segm_head:\n",
    "                head.num_classes = len(classes)\n",
    "\n",
    "\n",
    "if 'roi_head' in cfg.model:\n",
    "    if 'semantic_roi_extractor' in cfg.model.roi_head:\n",
    "        cfg.model.roi_head.semantic_roi_extractor = None\n",
    "    if 'semantic_head' in cfg.model.roi_head:\n",
    "        cfg.model.roi_head.semantic_head = None\n",
    "\n",
    "\n",
    "# Put gt boxes in CPU\n",
    "# for i in cfg.model.train_cfg:\n",
    "#     if 'assigner' in i:\n",
    "#         i.assigner.gpu_assign_thr = 0\n",
    "#         i.assigner.pos_iou_thr = 0\n",
    "\n",
    "# Use checkpoint to save memory (but slower)\n",
    "cfg.model.backbone.with_cp = True\n",
    "\n",
    "# FP16\n",
    "cfg.fp16 = dict(loss_scale='dynamic')\n",
    "\n",
    "# Load pretrained weights\n",
    "cfg.load_from = hyperparameters['WEIGHTS']\n",
    "\n",
    "# Set up working dir to save files and logs\n",
    "cfg.work_dir = work_dir\n",
    "\n",
    "# Save weights for the epoch with best metrics\n",
    "cfg.default_hooks.checkpoint.interval = -1\n",
    "cfg.default_hooks.checkpoint.save_best = 'coco/bbox_mAP'\n",
    "# cfg.default_hooks.checkpoint.save_best = 'coco/segm_mAP'\n",
    "# Frequency for logging\n",
    "cfg.default_hooks.logger.interval = 100\n",
    "\n",
    "cfg.param_scheduler = [\n",
    "    # LR\n",
    "    dict(type='CosineAnnealingLR',\n",
    "        T_max = round(hyperparameters['WARMUP_PCT'] * hyperparameters['EPOCHS']),\n",
    "        eta_min = hyperparameters['LR'],\n",
    "        begin = 0,\n",
    "        end = round(hyperparameters['WARMUP_PCT'] * hyperparameters['EPOCHS']),\n",
    "        by_epoch=True,\n",
    "        convert_to_iter_based=True),\n",
    "    dict(type='CosineAnnealingLR',\n",
    "        T_max = hyperparameters['EPOCHS'] - round(hyperparameters['WARMUP_PCT'] * hyperparameters['EPOCHS']) - 1,\n",
    "        eta_min = hyperparameters['LR'] / hyperparameters['LR_DIVS'][1],\n",
    "        begin = round(hyperparameters['WARMUP_PCT'] * hyperparameters['EPOCHS']),\n",
    "        end = hyperparameters['EPOCHS'] - 1,\n",
    "        by_epoch=True,\n",
    "        convert_to_iter_based=True),\n",
    "]\n",
    "\n",
    "# Initial lr and momentum\n",
    "cfg.optim_wrapper.optimizer.lr = hyperparameters['LR'] / hyperparameters['LR_DIVS'][0]\n",
    "# cfg.optim_wrapper.optimizer.momentum = hyperparameters['MOMS'][1]\n",
    "cfg.optim_wrapper.optimizer.type = 'AdamW'\n",
    "cfg.optim_wrapper.optimizer.weight_decay = 0.05\n",
    "if 'momentum' in cfg.optim_wrapper.optimizer:\n",
    "    del cfg.optim_wrapper.optimizer.momentum\n",
    "\n",
    "# Set seed\n",
    "cfg.seed = hyperparameters['SEED']\n",
    "set_random_seed(hyperparameters['SEED'], deterministic=False)\n",
    "\n",
    "# We can also use tensorboard to log the training process\n",
    "cfg.visualizer.vis_backends.append({\"type\":'TensorboardVisBackend'})\n",
    "\n",
    "# Gradient clipping\n",
    "cfg.optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
    "\n",
    "# Batch size\n",
    "cfg.train_dataloader.batch_size = hyperparameters['BS']\n",
    "cfg.val_dataloader.batch_size = hyperparameters['BS']\n",
    "cfg.test_dataloader.batch_size = hyperparameters['BS']\n",
    "\n",
    "# Epochs\n",
    "cfg.train_cfg = dict(max_epochs=hyperparameters['EPOCHS'], type='EpochBasedTrainLoop', val_interval=1)\n",
    "\n",
    "# Validation loss log\n",
    "cfg.custom_hooks = [dict(type='ValidationLossHook',\n",
    "                    loss_name='total_loss',       # Name for logging\n",
    "                    calculation_method='loss',    # Explicitly use model.loss()\n",
    "                    force_train_mode=True,        # Explicitly force train()\n",
    "                    sum_all_components=True,      # Explicitly sum all\n",
    "                    interval=1)]\n",
    "\n",
    "# ------------------ DATA AUGMENTATION CONFIG ------------------\n",
    "\n",
    "# Update train pipeline\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True, with_mask=False, with_seg=False, poly2mask=False),\n",
    "] + hyperparameters['TRANSFORMS'] + [dict(type='PackDetInputs'),]\n",
    "\n",
    "# Update valid pipeline\n",
    "valid_pipeline=[\n",
    "            dict(type='LoadImageFromFile'),\n",
    "            dict(type='Resize', keep_ratio=True, scale=hyperparameters['IMG_SIZE']),\n",
    "            dict(type='LoadAnnotations', with_bbox=True),\n",
    "            dict(\n",
    "                meta_keys=(\n",
    "                    'img_id',\n",
    "                    'img_path',\n",
    "                    'ori_shape',\n",
    "                    'img_shape',\n",
    "                    'scale_factor',\n",
    "                ),\n",
    "                type='PackDetInputs'),\n",
    "]\n",
    "\n",
    "# Set all pipelines\n",
    "cfg.train_dataloader.dataset.pipeline = train_pipeline\n",
    "cfg.val_dataloader.dataset.pipeline = valid_pipeline\n",
    "cfg.test_dataloader.dataset.pipeline = valid_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.mmdetection.visualization import show_samples\n",
    "\n",
    "# Show samples\n",
    "show_samples(cfg, unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c583dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show transforms\n",
    "show_samples(cfg, unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ffd23",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Build the runner from config\n",
    "runner = Runner.from_cfg(cfg)\n",
    "# Start training\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dceef",
   "metadata": {},
   "source": [
    "# Results and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Get folder where the files are saved\n",
    "work_subdir = max([work_dir + d for d in os.listdir(work_dir) if os.path.isdir(work_dir + d)], key=os.path.getmtime) + '/'\n",
    "# Move model and log to the rigth folder\n",
    "config_path = work_subdir + 'vis_data/' + 'config.py'\n",
    "model_path = work_dir + [i for i in os.listdir(work_dir) if 'best' in i][0]\n",
    "os.rename(config_path, work_subdir+'config.py')\n",
    "os.rename(model_path, work_subdir+[i for i in os.listdir(work_dir) if 'best' in i][0])\n",
    "config_path = work_subdir+'config.py'\n",
    "model_path = work_subdir+[i for i in os.listdir(work_subdir) if 'best' in i][0]\n",
    "# Delete unnecesary files\n",
    "shutil.rmtree(work_subdir+'vis_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee754d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.mmdetection.plots import plot_losses, plot_metrics\n",
    "from DLOlympus.mmdetection.utils import get_metrics\n",
    "\n",
    "log_path = work_subdir + work_subdir.split('/')[-2] + '.log'\n",
    "\n",
    "_ = plot_losses(log_path, work_subdir)\n",
    "_ = plot_metrics(log_path, work_subdir)\n",
    "results = get_metrics(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20c6d7",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {mmdet_root}\n",
    "\n",
    "# Save results\n",
    "!python tools/test.py \\\n",
    "    {config_path} \\\n",
    "    {model_path} \\\n",
    "    --out {work_subdir+'results.pkl'} \\\n",
    "    --cfg-options custom_hooks=\"[]\"\n",
    "    # --show \\\n",
    "    # --show-dir {work_subdir+'preds'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.mmdetection.evaluation import collect_detections_and_ground_truths\n",
    "from mmdet.utils import replace_cfg_vals, update_data_root\n",
    "from mmdet.registry import DATASETS\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmengine.fileio import load\n",
    "\n",
    "cfg = replace_cfg_vals(cfg)\n",
    "update_data_root(cfg)\n",
    "init_default_scope(cfg.get('default_scope', 'mmdet'))\n",
    "results = load(work_subdir+'results.pkl')\n",
    "dataset = DATASETS.build(cfg.test_dataloader.dataset)\n",
    "\n",
    "pred_classes, gt_classes = collect_detections_and_ground_truths(dataset, results, score_thr=0.3, tp_iou_thr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLOlympus.utils.plots import plot_confusion_matrix\n",
    "import itertools\n",
    "\n",
    "new_vocab = [' '.join(i) for i in list(itertools.product(list(classes)+['background']))]\n",
    "_ = plot_confusion_matrix(gt_classes, pred_classes, new_vocab, work_subdir+'confusion.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
